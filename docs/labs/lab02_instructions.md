# Lab 02: Complexity and the Data Flow

Welcome to the second Big Data laboratory session! In this lab, we will leave the safety of small datasets and enter the "Red Zone" where algorithmic complexity matters.

## ðŸ“š Additional Resources

- **[Tips & Reference Guide](lab02_guide.md)** - detailed tips, code examples, and cheatsheets for every exercise.
- **[Lab 01 Instructions](lab01_setup_io.md)** - if you need to review setup or I/O basics.

## ðŸŽ¯ What You Will Learn

- **The Scale Factor**: Why code that works for 1,000 items fails for 1,000,000.
- **Memory Hierarchy**: Proving via benchmarks that RAM is faster than Disk.
- **Profiling Tools**: How to use `cProfile`, `py-spy` flamegraphs, and `line_profiler` to find bottlenecks.
- **Optimization**: Converting O(NÂ²) algorithms to O(N) for massive speedups.

## âœ… Pre-flight Checklist

Before starting, ensure you have:

1.  **Completed Lab 01**: You understand basic I/O and have your environment set up.
2.  **Updated your repo**: Run `git pull` to get the latest changes (if applicable).
3.  **Installed dependencies**: Run `uv sync` to ensure you have `psutil` (used for memory profiling).
4.  **Install profiling tools** (for Exercise 3):
    ```bash
    pip install py-spy line_profiler
    ```

---

## ðŸ“ Lab Steps

Follow along in the notebook `notebooks/lab02_complexity_dataflow.ipynb`.

### A. Generate the Dataset (The Red Zone ðŸ”´)

We need a dataset large enough to break inefficient code. You will implement `generate_user_logs()` to create:

-   **Rows**: 1,000,000
-   **Columns**: `user_id`, `session_id`, `action`, `timestamp`, `value`
-   **Features**: Duplicate user IDs (essential for later exercises)

**Goal**: Save this as `data/raw/user_logs_1m.csv`.

### B. Exercise 1: Search & Sort Efficiency

You will compare the performance impact of different data structures and algorithms.

**Part 1A: Search - O(N) vs O(1)**

Compare finding an item in a Python **List** versus a **Set**.

1.  Create a list of 1M numbers.
2.  Create a set of the same 1M numbers.
3.  Implement `benchmark_search()` to search for 1,000 random keys in both.
4.  Calculate the speedup.

**What to expect**: The Set should be ~1000x faster.

**Part 1B: Sort - O(NÂ²) vs O(N log N)**

Compare sorting algorithms at different scales.

1.  Implement `bubble_sort()` - the classic O(NÂ²) algorithm.
2.  Compare against Python's built-in `sorted()` (Timsort, O(N log N)).
3.  Run benchmarks at N = 100, 1000, 5000.
4.  Predict and verify what happens at N = 10,000.

**What to expect**: Python sort should be 100x+ faster at N=5000.

### C. Exercise 2: The Data Flow (Memory Hierarchy)

You will implement three functions to load data, each representing a different approach:

1.  **`load_full()`**: Read the whole CSV into RAM (`pd.read_csv()`). Fastest, but requires plenty of RAM.
2.  **`load_chunked()`**: Read in blocks of 50k rows. Slower, but constant memory usage.
3.  **`load_iterator()`**: Read line-by-line. Slowest, but minimal memory.

**Goal**: Understand the trade-off between **Speed** and **Memory**.

### D. Exercise 3: Identifying the Bottleneck ðŸ”¥

We provide a function `find_duplicates_slow()` that is deliberately terrible (O(NÂ²)).

**Part 3A: cProfile Analysis**

1.  Run `find_duplicates_slow()` on a small sample (10k rows).
2.  Implement `profile_function()` using Python's `cProfile`.
3.  Analyze the output to find *exactly* which functions consume the most time.

**Part 3B: Flamegraph Visualization**

1.  Install `py-spy`: `pip install py-spy`
2.  Generate a flamegraph SVG of the slow function.
3.  Open in browser and identify the widest bar (= bottleneck).
4.  Compare with cProfile results - do they match?

**Part 3C: Line-by-Line Profiling**

1.  Use `line_profiler` to profile the slow function line by line.
2.  Identify the exact lines that consume the most time.
3.  Explain WHY those specific lines are slow.

**Goal**: Learn that you can't optimize what you can't measure.

### E. Exercise 4: The 10x Challenge ðŸ†

Your task is to refactor the slow function to be at least **10x faster**.

1.  Implement `find_duplicates_fast()`.
2.  **Strategy**: Use a Hash Map (Dictionary or `collections.Counter`) to count items in O(N).
3.  **Bonus**: Use Sorting (O(N log N)).

**Goal**: Prove that Algorithms > Hardware. A better algorithm on a laptop beats a bad algorithm on a supercomputer.

---

## ðŸ“¦ What to Submit

Submit **exactly these two files**:

1.  **`notebooks/lab02_complexity_dataflow.ipynb`** â€” Your completed notebook.
2.  **`results/lab02_metrics.json`** â€” The JSON file generated by the notebook.

**Do NOT submit:**
-   The 1M row CSV file (it's ~60-80MB).
-   The `__pycache__` directories.

---

## ðŸš€ Next Steps

After completing this lab:

1.  Check your `results/lab02_metrics.json`.
2.  Write your reflection in the notebook.
3.  Submit your work!

**Questions?** Check the [Tips & Reference Guide](lab02_guide.md) or ask your instructor.
