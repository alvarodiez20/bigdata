{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: Out-of-Core, Streaming & Parallel Processing\n",
    "\n",
    "**Course:** Big Data\n",
    "\n",
    "---\n",
    "\n",
    "## Student Information\n",
    "\n",
    "**Name:** `Your Name Here`\n",
    "\n",
    "**Date:** `DD/MM/YYYY`\n",
    "\n",
    "---\n",
    "\n",
    "**Goal:** Process datasets larger than RAM using chunking, implement streaming statistics, and leverage parallelization (threading/multiprocessing) for performance.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Use PyArrow Directly**: Understand when to use PyArrow vs Pandas for I/O\n",
    "2. **Apply Projection Pushdown**: Read only the columns you need\n",
    "3. **Process Out-of-Core**: Handle datasets larger than RAM with chunking\n",
    "4. **Implement Online Statistics**: Compute mean/std in a single pass (Welford's algorithm)\n",
    "5. **Parallelize Work**: Use threading for I/O and multiprocessing for CPU-bound tasks\n",
    "6. **Build Complete Pipelines**: Combine chunking + parallelization\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Fill in your information above** before starting the lab\n",
    "2. Read each cell carefully before running it\n",
    "3. Implement the **TODO functions** when you see them\n",
    "4. Run cells **from top to bottom** (Shift+Enter)\n",
    "5. Check that output makes sense after each cell\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used in This Lab\n",
    "\n",
    "- **`pyarrow`** — Direct Parquet reading and Arrow Table operations\n",
    "- **`pandas`** — DataFrame operations and chunked CSV reading\n",
    "- **`numpy`** — Numerical operations\n",
    "- **`concurrent.futures`** — ThreadPoolExecutor and ProcessPoolExecutor\n",
    "- **`psutil`** — Memory monitoring\n",
    "- **`matplotlib`** — Plotting memory and speedup charts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Imports and Setup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyArrow version: {pa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Define Paths"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW = Path(\"../data/raw\")\n",
    "DATA_PROCESSED = Path(\"../data/processed\")\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "\n",
    "WARMUP_PARQUET = DATA_PROCESSED / \"sales_warmup.parquet\"\n",
    "SALES_CSV = DATA_RAW / \"sales_large.csv\"\n",
    "SALES_PARTITIONED = DATA_PROCESSED / \"sales_partitioned\"\n",
    "PARTITIONS_DIR = DATA_PROCESSED / \"partitions\"\n",
    "ELECTRONICS_PARQUET = DATA_PROCESSED / \"electronics_only.parquet\"\n",
    "METRICS_PATH = RESULTS_DIR / \"lab05_metrics.json\"\n",
    "\n",
    "for d in [DATA_RAW, DATA_PROCESSED, RESULTS_DIR, PARTITIONS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Paths defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 0: PyArrow Benchmark & Warm-up (15 min)\n",
    "\n",
    "**Objective**: Familiarize yourself with PyArrow by comparing its performance against Pandas, and understand `iter_batches()` that we will use in the rest of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: `generate_warmup_data()`\n",
    "\n",
    "Generate a warmup dataset (5M rows) and save as Parquet.\n",
    "\n",
    "**Columns**: `product_id`, `category`, `price`, `quantity`, `customer_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_warmup_data(n: int = 5_000_000, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a warmup dataset and save as Parquet.\n",
    "\n",
    "    Args:\n",
    "        n: Number of rows\n",
    "        seed: Random seed\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: product_id, category, price, quantity, customer_id\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Step 1: Set random seed with np.random.seed(seed)\n",
    "    # Step 2: Create DataFrame with:\n",
    "    #   - 'product_id': np.random.randint(1, 10000, n)\n",
    "    #   - 'category': np.random.choice(['Electronics','Clothing','Home','Sports','Food'], n)\n",
    "    #   - 'price': np.random.uniform(1, 1000, n).round(2)\n",
    "    #   - 'quantity': np.random.randint(1, 50, n)\n",
    "    #   - 'customer_id': np.random.randint(1, 100000, n)\n",
    "    # Step 3: Save to WARMUP_PARQUET with df.to_parquet(..., index=False)\n",
    "    # Step 4: Return df\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating warmup dataset (5M rows)...\")\n",
    "df_warmup = generate_warmup_data()\n",
    "if df_warmup is not None:\n",
    "    print(f\"Shape: {df_warmup.shape}\")\n",
    "    print(f\"Memory: {df_warmup.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "    print(df_warmup.head())\n",
    "else:\n",
    "    print(\"TODO: Implement generate_warmup_data()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: `benchmark_read_methods()`\n",
    "\n",
    "Compare three approaches to reading Parquet:\n",
    "- **Method A**: `pd.read_parquet()` — Pandas (with conversion overhead)\n",
    "- **Method B**: `pq.read_table()` — Arrow direct (no conversion)\n",
    "- **Method C**: `table.to_pandas()` — Measure conversion cost separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_read_methods() -> dict:\n",
    "    \"\"\"\n",
    "    Compare pd.read_parquet vs pq.read_table vs Arrow-to-Pandas conversion.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with timing results for each method.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Method A: start = time.perf_counter(); df = pd.read_parquet(WARMUP_PARQUET); ...\n",
    "    #   RAM: df.memory_usage(deep=True).sum() / 1e6\n",
    "    # Method B: table = pq.read_table(WARMUP_PARQUET); ...\n",
    "    #   RAM: table.nbytes / 1e6\n",
    "    # Method C: df = table.to_pandas(); ...\n",
    "    # Print results and compute speedup = t_pandas / t_arrow\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_bench = benchmark_read_methods()\n",
    "if read_bench is None:\n",
    "    print(\"TODO: Implement benchmark_read_methods()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why is reading as an Arrow Table faster than reading directly to Pandas, if Pandas uses Arrow internally?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: `benchmark_projection_pushdown()`\n",
    "\n",
    "Compare reading all columns vs only 2 columns (`price`, `quantity`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_projection_pushdown() -> dict:\n",
    "    \"\"\"\n",
    "    Compare reading all columns vs only needed columns from Parquet.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with timing, size, speedup, and total_revenue.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. Read ALL: table_all = pq.read_table(WARMUP_PARQUET)\n",
    "    # 2. Read 2 cols: table_cols = pq.read_table(WARMUP_PARQUET, columns=['price', 'quantity'])\n",
    "    # 3. Compute revenue: pc.multiply(table_cols.column('price'), table_cols.column('quantity'))\n",
    "    #    total = pc.sum(revenue).as_py()\n",
    "    # 4. Print speedup and data reduction\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_bench = benchmark_projection_pushdown()\n",
    "if proj_bench is None:\n",
    "    print(\"TODO: Implement benchmark_projection_pushdown()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: In what real-world cases would you leverage this pattern instead of reading the full DataFrame?\n",
    "\n",
    "*Your answer here:*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `iter_batches()` — The bridge between Arrow and out-of-core\n",
    "\n",
    "This function is what we will use for the rest of the lab for Parquet streaming. It is **already implemented** — study it and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_iter_batches(batch_size: int = 500_000) -> dict:\n",
    "    \"\"\"Use iter_batches() to process Parquet in streaming fashion (pre-filled).\"\"\"\n",
    "    pf = pq.ParquetFile(WARMUP_PARQUET)\n",
    "    print(f\"Row groups: {pf.metadata.num_row_groups}\")\n",
    "    print(f\"Total rows: {pf.metadata.num_rows:,}\")\n",
    "\n",
    "    total_revenue = 0\n",
    "    total_rows = 0\n",
    "    for i, batch in enumerate(pf.iter_batches(batch_size=batch_size, columns=['price', 'quantity'])):\n",
    "        print(f\"Batch {i}: type={type(batch).__name__}, rows={len(batch):,}\")\n",
    "        revenue = pc.multiply(batch.column('price'), batch.column('quantity'))\n",
    "        total_revenue += pc.sum(revenue).as_py()\n",
    "        total_rows += len(batch)\n",
    "\n",
    "    print(f\"\\nTotal rows processed: {total_rows:,}\")\n",
    "    print(f\"Total revenue: {total_revenue:,.0f}\")\n",
    "    print(f\"Memory per batch: ~{batch_size * 2 * 8 / 1e6:.1f} MB (2 float64 columns)\")\n",
    "    return {'total_revenue': total_revenue, 'total_rows': total_rows, 'num_batches': i + 1}\n",
    "\n",
    "batch_results = process_with_iter_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema inspection (pre-filled)\n",
    "\n",
    "Inspect a Parquet file's metadata **without reading any data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_parquet_schema() -> dict:\n",
    "    \"\"\"Inspect Parquet file metadata without reading data (pre-filled).\"\"\"\n",
    "    pf = pq.ParquetFile(WARMUP_PARQUET)\n",
    "    print(\"Schema:\")\n",
    "    print(pf.schema_arrow)\n",
    "    print(f\"\\nFile metadata:\")\n",
    "    print(f\"  Rows:       {pf.metadata.num_rows:,}\")\n",
    "    print(f\"  Row groups: {pf.metadata.num_row_groups}\")\n",
    "    print(f\"  Columns:    {pf.metadata.num_columns}\")\n",
    "    rg = pf.metadata.row_group(0)\n",
    "    col = rg.column(0)\n",
    "    print(f\"\\nStatistics for '{col.path_in_schema}':\")\n",
    "    if col.statistics:\n",
    "        print(f\"  Min: {col.statistics.min}\")\n",
    "        print(f\"  Max: {col.statistics.max}\")\n",
    "    return {'num_rows': pf.metadata.num_rows, 'num_row_groups': pf.metadata.num_row_groups}\n",
    "\n",
    "schema_info = inspect_parquet_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 0 — Summary Table\n",
    "\n",
    "| Operation | Approx. Time | Approx. RAM | When to use |\n",
    "|-----------|-------------|-------------|-------------|\n",
    "| `pd.read_parquet()` | ~1.0s | ~150 MB | When you need a full DataFrame |\n",
    "| `pq.read_table()` | ~0.5s | ~80 MB | When you operate in Arrow or convert later |\n",
    "| `pq.read_table(columns=[...])` | ~0.2s | ~20 MB | When you only need a few columns |\n",
    "| `iter_batches()` | same, streaming | ~10 MB/batch | When the file doesn't fit in RAM |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Out-of-Core Processing with Chunking (25 min)\n",
    "\n",
    "**Objective**: Process a dataset larger than RAM using chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: `generate_large_dataset()`\n",
    "\n",
    "Generate 20M rows and save as CSV + partitioned Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_large_dataset(n: int = 20_000_000, seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Generate a large dataset and save as CSV and partitioned Parquet.\n",
    "\n",
    "    Args:\n",
    "        n: Number of rows\n",
    "        seed: Random seed\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Step 1: np.random.seed(seed)\n",
    "    # Step 2: Create DataFrame with columns:\n",
    "    #   date, product_id, category, price, quantity, customer_id\n",
    "    # Step 3: df.to_csv(SALES_CSV, index=False)\n",
    "    # Step 4: df.to_parquet(SALES_PARTITIONED, partition_cols=['category'], index=False)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating large dataset (20M rows)...\")\n",
    "print(\"(This may take 2-5 minutes)\\n\")\n",
    "start = time.perf_counter()\n",
    "generate_large_dataset()\n",
    "print(f\"\\nCompleted in {time.perf_counter() - start:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### TODO 5: `chunked_statistics()`\n", "\n", "Calculate average price using chunking — **without loading the full file in RAM**."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_statistics(chunksize: int = 500_000) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate statistics using chunking over CSV.\n",
    "\n",
    "    Args:\n",
    "        chunksize: Number of rows per chunk\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with total_sum, total_count, and avg_price.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. Initialize total_sum = 0, total_count = 0\n",
    "    # 2. for chunk in pd.read_csv(SALES_CSV, chunksize=chunksize):\n",
    "    #      total_sum += chunk['price'].sum()\n",
    "    #      total_count += len(chunk)\n",
    "    # 3. avg_price = total_sum / total_count\n",
    "    # 4. Return {'total_sum': ..., 'total_count': ..., 'avg_price': ...}\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = chunked_statistics()\n",
    "if stats is None:\n",
    "    print(\"TODO: Implement chunked_statistics()\")\n",
    "else:\n",
    "    print(f\"Average price: {stats['avg_price']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### TODO 6: `chunked_filter_save()`\n", "\n", "Filter only \"Electronics\" sales and save to Parquet — processing chunk by chunk."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_filter_save(chunksize: int = 500_000) -> int:\n",
    "    \"\"\"\n",
    "    Filter and save only Electronics sales using chunking.\n",
    "\n",
    "    Args:\n",
    "        chunksize: Number of rows per chunk\n",
    "\n",
    "    Returns:\n",
    "        Number of Electronics rows saved.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. results = []\n",
    "    # 2. for chunk in pd.read_csv(SALES_CSV, chunksize=chunksize):\n",
    "    #      filtered = chunk[chunk['category'] == 'Electronics']\n",
    "    #      results.append(filtered)\n",
    "    # 3. electronics = pd.concat(results)\n",
    "    # 4. electronics.to_parquet(ELECTRONICS_PARQUET, index=False)\n",
    "    # 5. Return len(electronics)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_electronics = chunked_filter_save()\n",
    "if n_electronics is None:\n",
    "    print(\"TODO: Implement chunked_filter_save()\")\n",
    "else:\n",
    "    print(f\"Saved {n_electronics:,} Electronics rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Memory Monitoring (pre-filled)\n", "\n", "Monitor memory usage during chunked processing to prove it stays constant."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_memory_chunking(chunksize: int = 500_000) -> list:\n",
    "    \"\"\"Monitor memory usage during chunked processing (pre-filled).\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_usage = []\n",
    "    for i, chunk in enumerate(pd.read_csv(SALES_CSV, chunksize=chunksize)):\n",
    "        mem_mb = process.memory_info().rss / 1024**2\n",
    "        mem_usage.append(mem_mb)\n",
    "        chunk['price'].sum()\n",
    "    print(f\"Chunks processed: {len(mem_usage)}\")\n",
    "    print(f\"Memory min: {min(mem_usage):.1f} MB, max: {max(mem_usage):.1f} MB\")\n",
    "    print(f\"Variation: {max(mem_usage) - min(mem_usage):.1f} MB\")\n",
    "    return mem_usage\n",
    "\n",
    "mem = monitor_memory_chunking()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mem, 'b-o', markersize=3)\n",
    "plt.xlabel('Chunk number')\n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.title('Constant Memory with Chunking')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'memory_chunking.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Online Statistics — Welford's Algorithm (20 min)\n",
    "\n",
    "**Objective**: Implement streaming statistics that work on infinite data streams.\n",
    "\n",
    "### Welford's Algorithm\n",
    "```\n",
    "For each new value x:\n",
    "  count += 1\n",
    "  delta = x - mean\n",
    "  mean += delta / count\n",
    "  delta2 = x - mean       (uses UPDATED mean!)\n",
    "  M2 += delta * delta2\n",
    "\n",
    "variance = M2 / count\n",
    "std = sqrt(variance)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### TODO 7: `OnlineStats` class"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineStats:\n",
    "    \"\"\"Online (streaming) statistics using Welford's algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.mean = 0.0\n",
    "        self.M2 = 0.0\n",
    "        self.min_val = float('inf')\n",
    "        self.max_val = float('-inf')\n",
    "\n",
    "    def update(self, x):\n",
    "        \"\"\"Update statistics with a new value using Welford's algorithm.\"\"\"\n",
    "        # TODO: Implement Welford's algorithm\n",
    "        # 1. self.count += 1\n",
    "        # 2. delta = x - self.mean\n",
    "        # 3. self.mean += delta / self.count\n",
    "        # 4. delta2 = x - self.mean  (uses UPDATED mean!)\n",
    "        # 5. self.M2 += delta * delta2\n",
    "        # 6. Update min_val and max_val\n",
    "        pass\n",
    "\n",
    "    def variance(self):\n",
    "        \"\"\"Return the population variance.\"\"\"\n",
    "        # TODO: return self.M2 / self.count (handle count < 2)\n",
    "        pass\n",
    "\n",
    "    def std(self):\n",
    "        \"\"\"Return the population standard deviation.\"\"\"\n",
    "        # TODO: return sqrt(self.variance())\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Validate OnlineStats against NumPy (pre-filled)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_online_stats(n_rows: int = 100_000) -> dict:\n",
    "    \"\"\"Test OnlineStats against NumPy (pre-filled).\"\"\"\n",
    "    chunk = pd.read_csv(SALES_CSV, nrows=n_rows)\n",
    "    stats = OnlineStats()\n",
    "    for val in chunk['price'].values:\n",
    "        stats.update(val)\n",
    "    np_mean = chunk['price'].mean()\n",
    "    np_std = chunk['price'].std(ddof=0)\n",
    "    mean_match = np.isclose(stats.mean, np_mean)\n",
    "    std_match = np.isclose(stats.std(), np_std)\n",
    "    print(f\"OnlineStats mean: {stats.mean:.4f}  |  NumPy mean: {np_mean:.4f}  |  Match: {mean_match}\")\n",
    "    print(f\"OnlineStats std:  {stats.std():.4f}  |  NumPy std:  {np_std:.4f}  |  Match: {std_match}\")\n",
    "    assert mean_match, f\"Mean mismatch: {stats.mean} vs {np_mean}\"\n",
    "    assert std_match, f\"Std mismatch: {stats.std()} vs {np_std}\"\n",
    "    print(\"\\n✅ All assertions passed!\")\n",
    "    return {'online_mean': round(stats.mean, 4), 'numpy_mean': round(np_mean, 4)}\n",
    "\n",
    "validation = test_online_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Streaming stats on full dataset (pre-filled)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_stats_full(chunksize: int = 500_000) -> dict:\n",
    "    \"\"\"Compute statistics over full CSV using OnlineStats + chunking (pre-filled).\"\"\"\n",
    "    stats = OnlineStats()\n",
    "    for chunk in pd.read_csv(SALES_CSV, chunksize=chunksize):\n",
    "        for value in chunk['price'].values:\n",
    "            stats.update(value)\n",
    "    print(f\"Mean: {stats.mean:.4f}, Std: {stats.std():.4f}\")\n",
    "    print(f\"Min: {stats.min_val:.2f}, Max: {stats.max_val:.2f}, Count: {stats.count:,}\")\n",
    "    return {'mean': round(stats.mean, 4), 'std': round(stats.std(), 4), 'count': stats.count}\n",
    "\n",
    "print(\"Computing streaming statistics over full dataset...\")\n",
    "start = time.perf_counter()\n",
    "full_stats = streaming_stats_full()\n",
    "print(f\"Completed in {time.perf_counter() - start:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Practical Parallelization (25 min)\n",
    "\n",
    "**Objective**: Compare threading, multiprocessing, and sequential execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Create Partitions (pre-filled)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitions(n_partitions: int = 16) -> list:\n",
    "    \"\"\"Create partition files from the large dataset (pre-filled).\"\"\"\n",
    "    PARTITIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Reading dataset for partitioning...\")\n",
    "    df = pd.read_csv(SALES_CSV)\n",
    "    files = []\n",
    "    for i in range(n_partitions):\n",
    "        start_idx = i * len(df) // n_partitions\n",
    "        end_idx = (i + 1) * len(df) // n_partitions\n",
    "        filepath = PARTITIONS_DIR / f\"part_{i:03d}.parquet\"\n",
    "        df.iloc[start_idx:end_idx].to_parquet(filepath, index=False)\n",
    "        files.append(str(filepath))\n",
    "    print(f\"Created {n_partitions} partitions in {PARTITIONS_DIR}\")\n",
    "    return files\n",
    "\n",
    "partition_files = create_partitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: `benchmark_threading()`\n",
    "\n",
    "Compare sequential vs `ThreadPoolExecutor` for **reading** 16 partition files.\n",
    "\n",
    "Reading files is **I/O-bound** — threads help because the GIL is released during I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_threading(n_workers: int = 8) -> dict:\n",
    "    \"\"\"\n",
    "    Benchmark sequential vs threaded reading of partition files.\n",
    "\n",
    "    Args:\n",
    "        n_workers: Number of threads\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with sequential_sec, threaded_sec, speedup.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. files = sorted(glob.glob(str(PARTITIONS_DIR / '*.parquet')))\n",
    "    # 2. Sequential: dfs = [pd.read_parquet(f) for f in files]\n",
    "    # 3. Threaded: with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "    #        dfs = list(executor.map(pd.read_parquet, files))\n",
    "    # 4. Print times and speedup\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_bench = benchmark_threading()\n",
    "if thread_bench is None:\n",
    "    print(\"TODO: Implement benchmark_threading()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 9: `benchmark_multiprocessing()`\n",
    "\n",
    "Compare sequential vs `ProcessPoolExecutor` for **heavy computation**.\n",
    "\n",
    "CPU-bound work — processes help because each has its own GIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavy_process(filepath):\n",
    "    \"\"\"Process a partition: read, transform, aggregate.\"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df['score'] = np.sqrt(df['price']) * np.log1p(df['quantity'])\n",
    "    return df.groupby('category')['score'].agg(['mean', 'sum', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_multiprocessing(n_workers: int = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Benchmark sequential vs multiprocessing for heavy computation.\n",
    "\n",
    "    Args:\n",
    "        n_workers: Number of processes\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with sequential_sec, multiprocessing_sec, speedup.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. files = sorted(glob.glob(str(PARTITIONS_DIR / '*.parquet')))\n",
    "    # 2. Sequential: [heavy_process(f) for f in files]\n",
    "    # 3. Parallel: with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    #        list(executor.map(heavy_process, files))\n",
    "    # 4. Print times and speedup\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_bench = benchmark_multiprocessing()\n",
    "if proc_bench is None:\n",
    "    print(\"TODO: Implement benchmark_multiprocessing()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Worker Scaling Experiment (pre-filled)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_workers_scaling(max_workers: int = 8) -> dict:\n",
    "    \"\"\"Vary number of workers and measure speedup (pre-filled).\"\"\"\n",
    "    files = sorted(glob.glob(str(PARTITIONS_DIR / '*.parquet')))\n",
    "    start = time.time()\n",
    "    [heavy_process(f) for f in files]\n",
    "    seq_time = time.time() - start\n",
    "    speedups = {}\n",
    "    for n in [1, 2, 4, max_workers]:\n",
    "        start = time.time()\n",
    "        with ProcessPoolExecutor(max_workers=n) as ex:\n",
    "            list(ex.map(heavy_process, files))\n",
    "        speedups[n] = round(seq_time / (time.time() - start), 1)\n",
    "    for w, s in speedups.items():\n",
    "        print(f\"  {w} workers: {s}x speedup\")\n",
    "    return speedups\n",
    "\n",
    "print(\"Benchmarking worker scaling (this may take a few minutes)...\\n\")\n",
    "scaling = benchmark_workers_scaling()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(list(scaling.keys()), list(scaling.values()), 'bo-', linewidth=2, markersize=8)\n",
    "plt.plot([1, max(scaling.keys())], [1, max(scaling.keys())], 'r--', label='Ideal linear', alpha=0.7)\n",
    "plt.xlabel('Workers')\n",
    "plt.ylabel('Speedup')\n",
    "plt.legend()\n",
    "plt.title('Speedup vs Number of Workers')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'speedup_workers.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Complete Pipeline — Out-of-Core + Parallel (20 min)\n",
    "\n",
    "**Objective**: Combine chunking and parallelization in a real pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### `process_partition()` (pre-filled)\n", "\n", "This function processes a single partition — study it before implementing the pipeline."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(filepath):\n",
    "    \"\"\"Process a partition: read, transform, aggregate (pre-filled).\"\"\"\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df['revenue'] = df['price'] * df['quantity']\n",
    "    df['price_bin'] = pd.cut(df['price'], bins=[0, 50, 200, 500, 1000],\n",
    "                            labels=['low', 'mid', 'high', 'premium'])\n",
    "    return df.groupby(['category', 'price_bin'], observed=True).agg({\n",
    "        'revenue': ['sum', 'mean', 'count'],\n",
    "        'quantity': 'sum'\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 10: `run_parallel_pipeline()`\n",
    "\n",
    "Run the complete pipeline: process all partitions sequentially vs in parallel, then combine results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_pipeline(n_workers: int = 4) -> dict:\n",
    "    \"\"\"\n",
    "    Run the complete parallel pipeline and compare with sequential.\n",
    "\n",
    "    Args:\n",
    "        n_workers: Number of worker processes\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with sequential_sec, parallel_sec, speedup.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. files = sorted(glob.glob(str(PARTITIONS_DIR / '*.parquet')))\n",
    "    # 2. Sequential: results = [process_partition(f) for f in files]\n",
    "    #    final_seq = pd.concat(results).groupby(level=[0, 1]).sum()\n",
    "    # 3. Parallel: with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "    #    results = list(executor.map(process_partition, files))\n",
    "    #    final_par = pd.concat(results).groupby(level=[0, 1]).sum()\n",
    "    # 4. Print times, speedup, and final results\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running complete pipeline (sequential vs parallel)...\\n\")\n",
    "pipeline_results = run_parallel_pipeline()\n",
    "if pipeline_results is None:\n",
    "    print(\"TODO: Implement run_parallel_pipeline()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Reflection\n",
    "\n",
    "**Your task:** Write a short reflection (3-5 sentences) answering:\n",
    "\n",
    "1. What was the difference in speed between `pq.read_table()` and `pd.read_parquet()`? Why?\n",
    "2. How did chunking affect memory usage during processing?\n",
    "3. When would you use threading vs multiprocessing in your own data pipelines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your reflection here\n",
    "reflection = \"\"\"\n",
    "Replace this text with your reflection.\n",
    "Think about what you learned about out-of-core processing and parallelization.\n",
    "What will you do differently in your future data pipelines?\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Your reflection:\")\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n", "\n", "## 6. Save Results"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"lab\": \"05_outofcore_parallel\",\n",
    "    \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "    \"exercise_0\": {\n",
    "        \"read_benchmark\": read_bench if 'read_bench' in dir() and read_bench else None,\n",
    "        \"projection\": proj_bench if 'proj_bench' in dir() and proj_bench else None,\n",
    "    },\n",
    "    \"exercise_1\": {\n",
    "        \"chunked_stats\": stats if 'stats' in dir() and stats else None,\n",
    "        \"electronics_rows\": n_electronics if 'n_electronics' in dir() and n_electronics else None,\n",
    "    },\n",
    "    \"exercise_2\": {\n",
    "        \"validation\": validation if 'validation' in dir() and validation else None,\n",
    "        \"full_stats\": full_stats if 'full_stats' in dir() and full_stats else None,\n",
    "    },\n",
    "    \"exercise_3\": {\n",
    "        \"threading\": thread_bench if 'thread_bench' in dir() and thread_bench else None,\n",
    "        \"multiprocessing\": proc_bench if 'proc_bench' in dir() and proc_bench else None,\n",
    "        \"scaling\": scaling if 'scaling' in dir() and scaling else None,\n",
    "    },\n",
    "    \"exercise_4\": pipeline_results if 'pipeline_results' in dir() and pipeline_results else None,\n",
    "    \"reflection\": reflection,\n",
    "}\n",
    "\n",
    "with open(METRICS_PATH, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✅ Results saved to {METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab Complete!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **PyArrow > Pandas for I/O** — `pq.read_table()` avoids conversion overhead\n",
    "2. **Projection pushdown** — read only needed columns, save time and memory\n",
    "3. **Chunking keeps memory constant** — process any file size with fixed RAM\n",
    "4. **Welford's algorithm** — compute statistics in a single pass\n",
    "5. **Threading for I/O, multiprocessing for CPU** — choose the right tool\n",
    "6. **Amdahl's Law** — speedup is limited by the sequential portion\n",
    "\n",
    "### Files to Submit\n",
    "\n",
    "1. `notebooks/lab05_outofcore_parallel.ipynb` (this notebook)\n",
    "2. `results/lab05_metrics.json`\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
