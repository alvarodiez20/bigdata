{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 06: Out-of-Core, Streaming & Parallel Processing\n",
                "\n",
                "**Course:** Big Data\n",
                "\n",
                "---\n",
                "\n",
                "## Student Information\n",
                "\n",
                "**Name:** `Your Name Here`\n",
                "\n",
                "**Date:** `DD/MM/YYYY`\n",
                "\n",
                "---\n",
                "\n",
                "**Goal:** Process datasets larger than RAM using chunking, implement streaming statistics, and leverage parallelization (threading/multiprocessing) for performance.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this lab, you will be able to:\n",
                "\n",
                "1. **Use PyArrow Directly**: Understand when to use PyArrow vs Pandas for I/O\n",
                "2. **Apply Projection Pushdown**: Read only the columns you need\n",
                "3. **Process Out-of-Core**: Handle datasets larger than RAM with chunking\n",
                "4. **Implement Online Statistics**: Compute mean/std in a single pass (Welford's algorithm)\n",
                "5. **Parallelize Work**: Use threading for I/O and multiprocessing for CPU-bound tasks\n",
                "6. **Build Complete Pipelines**: Combine chunking + parallelization\n",
                "\n",
                "## Instructions\n",
                "\n",
                "1. **Fill in your information above** before starting the lab\n",
                "2. Read each cell carefully before running it\n",
                "3. Implement the **TODO functions** when you see them\n",
                "4. Run cells **from top to bottom** (Shift+Enter)\n",
                "5. Check that output makes sense after each cell\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Libraries Used in This Lab\n",
                "\n",
                "### Core Libraries\n",
                "\n",
                "- **`pyarrow`** — Direct Parquet reading and Arrow Table operations\n",
                "- **`pandas`** — DataFrame operations and chunked CSV reading\n",
                "- **`numpy`** — Numerical operations\n",
                "- **`concurrent.futures`** — ThreadPoolExecutor and ProcessPoolExecutor\n",
                "- **`psutil`** — Memory monitoring\n",
                "- **`matplotlib`** — Plotting memory and speedup charts\n",
                "\n",
                "### Why This Matters\n",
                "\n",
                "Real-world datasets often don't fit in RAM. This lab teaches three strategies:\n",
                "\n",
                "1. **Chunking**: Process data in pieces, one at a time\n",
                "2. **Streaming statistics**: Compute results without storing all data\n",
                "3. **Parallelization**: Use multiple cores to process faster\n",
                "\n",
                "Combined, these allow processing datasets of any size.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import time\n",
                "import os\n",
                "import glob\n",
                "from pathlib import Path\n",
                "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pyarrow as pa\n",
                "import pyarrow.parquet as pq\n",
                "import pyarrow.compute as pc\n",
                "import psutil\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(\"Imports successful!\")\n",
                "print(f\"Pandas version: {pd.__version__}\")\n",
                "print(f\"NumPy version: {np.__version__}\")\n",
                "print(f\"PyArrow version: {pa.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base directories\n",
                "DATA_RAW = Path(\"../data/raw\")\n",
                "DATA_PROCESSED = Path(\"../data/processed\")\n",
                "RESULTS_DIR = Path(\"../results\")\n",
                "\n",
                "# File paths for this lab\n",
                "WARMUP_PARQUET = DATA_PROCESSED / \"sales_warmup.parquet\"\n",
                "SALES_CSV = DATA_RAW / \"sales_large.csv\"\n",
                "SALES_PARTITIONED = DATA_PROCESSED / \"sales_partitioned\"\n",
                "PARTITIONS_DIR = DATA_PROCESSED / \"partitions\"\n",
                "ELECTRONICS_PARQUET = DATA_PROCESSED / \"electronics_only.parquet\"\n",
                "METRICS_PATH = RESULTS_DIR / \"lab06_metrics.json\"\n",
                "\n",
                "# Ensure directories exist\n",
                "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
                "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "PARTITIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(\"Paths defined:\")\n",
                "print(f\"  Warmup Parquet: {WARMUP_PARQUET}\")\n",
                "print(f\"  Sales CSV: {SALES_CSV}\")\n",
                "print(f\"  Partitions: {PARTITIONS_DIR}\")\n",
                "print(f\"  Metrics: {METRICS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Exercise 0: PyArrow en la Práctica — Benchmark y Warm-up (15 min)\n",
                "\n",
                "**Objetivo**: Familiarizarse con PyArrow comparando su rendimiento frente a Pandas puro, y entender `iter_batches()` que usaremos en el resto del lab."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 1: `generate_warmup_data()`\n",
                "\n",
                "Generate a warmup dataset (5M rows) and save as Parquet.\n",
                "\n",
                "**Hints:**\n",
                "- Use `np.random.seed(seed)` for reproducibility\n",
                "- Create columns: `product_id`, `category`, `price`, `quantity`, `customer_id`\n",
                "- Save with `df.to_parquet(WARMUP_PARQUET, index=False)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_warmup_data(n: int = 5_000_000, seed: int = 42) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Generate a warmup dataset and save as Parquet.\n",
                "\n",
                "    Args:\n",
                "        n: Number of rows\n",
                "        seed: Random seed\n",
                "\n",
                "    Returns:\n",
                "        DataFrame with columns: product_id, category, price, quantity, customer_id\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # Step 1: Set random seed with np.random.seed(seed)\n",
                "    # Step 2: Create DataFrame with columns:\n",
                "    #   - 'product_id': np.random.randint(1, 10000, n)\n",
                "    #   - 'category': np.random.choice(['Electronics','Clothing','Home','Sports','Food'], n)\n",
                "    #   - 'price': np.random.uniform(1, 1000, n).round(2)\n",
                "    #   - 'quantity': np.random.randint(1, 50, n)\n",
                "    #   - 'customer_id': np.random.randint(1, 100000, n)\n",
                "    # Step 3: Save to WARMUP_PARQUET with df.to_parquet(..., index=False)\n",
                "    # Step 4: Print memory usage and return df\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate warmup data\n",
                "print(\"Generating warmup dataset (5M rows)...\")\n",
                "df_warmup = generate_warmup_data()\n",
                "\n",
                "if df_warmup is not None:\n",
                "    print(f\"Shape: {df_warmup.shape}\")\n",
                "    print(f\"\\nSample:\")\n",
                "    print(df_warmup.head())\n",
                "else:\n",
                "    print(\"TODO: Implement generate_warmup_data()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tarea 1 — Benchmark: pd.read_parquet vs pyarrow directo\n",
                "\n",
                "### TODO 2: `benchmark_read_methods()`\n",
                "\n",
                "Compare three approaches to read the Parquet file:\n",
                "- **Método A**: `pd.read_parquet()` (Pandas, with overhead)\n",
                "- **Método B**: `pq.read_table()` (PyArrow direct, no conversion)\n",
                "- **Método C**: Arrow Table → Pandas (measure conversion cost)\n",
                "\n",
                "**Hints:**\n",
                "- Use `time.perf_counter()` for precise timing\n",
                "- Use `table.nbytes` for Arrow memory usage\n",
                "- Use `df.memory_usage(deep=True).sum()` for Pandas memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_read_methods() -> dict:\n",
                "    \"\"\"\n",
                "    Compare pd.read_parquet vs pq.read_table vs Arrow→Pandas conversion.\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with timing results for each method.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # Método A: pd.read_parquet(WARMUP_PARQUET)\n",
                "    #   - Measure time and RAM (df.memory_usage(deep=True).sum() / 1e6)\n",
                "    #\n",
                "    # Método B: pq.read_table(WARMUP_PARQUET)\n",
                "    #   - Measure time and RAM (table.nbytes / 1e6)\n",
                "    #\n",
                "    # Método C: table.to_pandas()\n",
                "    #   - Measure conversion time\n",
                "    #\n",
                "    # Print results and compute speedup\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run benchmark\n",
                "read_bench = benchmark_read_methods()\n",
                "\n",
                "if read_bench is None:\n",
                "    print(\"TODO: Implement benchmark_read_methods()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Pregunta**: ¿Por qué leer como Arrow Table es más rápido que leer directo a Pandas, si Pandas usa Arrow internamente?\n",
                "\n",
                "*Tu respuesta aquí:*\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tarea 2 — Projection pushdown: leer solo las columnas que necesitas\n",
                "\n",
                "### TODO 3: `benchmark_projection_pushdown()`\n",
                "\n",
                "Compare reading all columns vs only 2 columns (`price`, `quantity`).\n",
                "\n",
                "**Hints:**\n",
                "- Use `pq.read_table(path, columns=['price', 'quantity'])` for selective read\n",
                "- Use `pyarrow.compute.multiply()` and `pc.sum()` for Arrow-native computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_projection_pushdown() -> dict:\n",
                "    \"\"\"\n",
                "    Compare reading all columns vs only needed columns from Parquet.\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with timing and size results.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Read ALL columns: pq.read_table(WARMUP_PARQUET)\n",
                "    #    - Measure time and table.nbytes\n",
                "    #\n",
                "    # 2. Read ONLY 2 columns: pq.read_table(WARMUP_PARQUET, columns=['price', 'quantity'])\n",
                "    #    - Measure time and table.nbytes\n",
                "    #\n",
                "    # 3. Calculate revenue with Arrow:\n",
                "    #    revenue = pc.multiply(table.column('price'), table.column('quantity'))\n",
                "    #    total = pc.sum(revenue).as_py()\n",
                "    #\n",
                "    # 4. Print speedup and data reduction\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run projection pushdown benchmark\n",
                "proj_bench = benchmark_projection_pushdown()\n",
                "\n",
                "if proj_bench is None:\n",
                "    print(\"TODO: Implement benchmark_projection_pushdown()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Pregunta**: ¿En qué casos reales aprovecharías este patrón en lugar de leer el DataFrame completo?\n",
                "\n",
                "*Tu respuesta aquí:*\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tarea 3 — `iter_batches()`: el puente entre Arrow y out-of-core\n",
                "\n",
                "Esta función es la que usaremos en el resto del lab para chunking con Parquet.\n",
                "\n",
                "### TODO 4: `process_with_iter_batches()`\n",
                "\n",
                "**Hints:**\n",
                "- Create a `ParquetFile` with `pq.ParquetFile(WARMUP_PARQUET)`\n",
                "- Read metadata: `pf.metadata.num_row_groups`, `pf.metadata.num_rows`\n",
                "- Iterate: `pf.iter_batches(batch_size=500_000, columns=['price', 'quantity'])`\n",
                "- Each `batch` is a `RecordBatch`, NOT a DataFrame"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_with_iter_batches(batch_size: int = 500_000) -> dict:\n",
                "    \"\"\"\n",
                "    Use iter_batches() to process the Parquet file in streaming fashion.\n",
                "\n",
                "    Args:\n",
                "        batch_size: Number of rows per batch\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with total revenue, total rows, and batch info.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Open ParquetFile: pf = pq.ParquetFile(WARMUP_PARQUET)\n",
                "    # 2. Print metadata (row groups, total rows)\n",
                "    # 3. Iterate: for batch in pf.iter_batches(batch_size=..., columns=[...]):\n",
                "    #    - Calculate revenue per batch using pc.multiply and pc.sum\n",
                "    #    - Accumulate total_revenue and total_rows\n",
                "    # 4. Print results and return dict\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Process with iter_batches\n",
                "batch_results = process_with_iter_batches()\n",
                "\n",
                "if batch_results is None:\n",
                "    print(\"TODO: Implement process_with_iter_batches()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tarea 4 (opcional) — Schema inspection\n",
                "\n",
                "### TODO 5: `inspect_parquet_schema()`\n",
                "\n",
                "Inspect a Parquet file's metadata **without reading any data**.\n",
                "\n",
                "**Hints:**\n",
                "- `pf.schema_arrow` — Column types\n",
                "- `pf.metadata.num_rows`, `num_row_groups`, `num_columns`\n",
                "- `pf.metadata.row_group(0).column(0).statistics` — Min/max per column"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def inspect_parquet_schema() -> dict:\n",
                "    \"\"\"\n",
                "    Inspect Parquet file metadata without reading data.\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with schema information.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Open ParquetFile: pf = pq.ParquetFile(WARMUP_PARQUET)\n",
                "    # 2. Print schema: pf.schema_arrow\n",
                "    # 3. Print metadata: num_rows, num_row_groups, num_columns\n",
                "    # 4. Print statistics of first column in first row group\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inspect schema\n",
                "schema_info = inspect_parquet_schema()\n",
                "\n",
                "if schema_info is None:\n",
                "    print(\"TODO: Implement inspect_parquet_schema()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 0 — Summary Table\n",
                "\n",
                "| Operación | Tiempo aprox. | RAM aprox. | ¿Cuándo usarla? |\n",
                "|-----------|---------------|------------|------------------|\n",
                "| `pd.read_parquet()` | ~1.0s | ~150 MB | Cuando necesitas DataFrame completo |\n",
                "| `pq.read_table()` | ~0.5s | ~80 MB | Cuando operas en Arrow o conviertes después |\n",
                "| `pq.read_table(columns=[...])` | ~0.2s | ~20 MB | Cuando solo necesitas pocas columnas |\n",
                "| `iter_batches()` | misma, streaming | ~10 MB/batch | Cuando el archivo no cabe en RAM → Ejercicio 1 |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercise 1: Out-of-Core Processing with Chunking (25 min)\n",
                "\n",
                "**Objetivo**: Procesar dataset mayor que RAM usando chunking."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 6: `generate_large_dataset()`\n",
                "\n",
                "Generate 20M rows and save as CSV + partitioned Parquet.\n",
                "\n",
                "**Hints:**\n",
                "- Columns: `date`, `product_id`, `category`, `price`, `quantity`, `customer_id`\n",
                "- Use `df.to_csv(SALES_CSV, index=False)` for CSV\n",
                "- Use `df.to_parquet(SALES_PARTITIONED, partition_cols=['category'], index=False)` for partitioned Parquet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_large_dataset(n: int = 20_000_000, seed: int = 42) -> None:\n",
                "    \"\"\"\n",
                "    Generate a large dataset and save as CSV and partitioned Parquet.\n",
                "\n",
                "    Args:\n",
                "        n: Number of rows\n",
                "        seed: Random seed\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # Step 1: Set random seed\n",
                "    # Step 2: Create DataFrame with:\n",
                "    #   - 'date': pd.date_range('2020-01-01', periods=n, freq='s')\n",
                "    #   - 'product_id': np.random.randint(1, 10000, n)\n",
                "    #   - 'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Sports', 'Food'], n)\n",
                "    #   - 'price': np.random.uniform(1, 1000, n).round(2)\n",
                "    #   - 'quantity': np.random.randint(1, 50, n)\n",
                "    #   - 'customer_id': np.random.randint(1, 100000, n)\n",
                "    # Step 3: Save as CSV to SALES_CSV\n",
                "    # Step 4: Save as partitioned Parquet to SALES_PARTITIONED\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate large dataset\n",
                "# WARNING: This may take several minutes and use significant disk space\n",
                "print(\"Generating large dataset (20M rows)...\")\n",
                "print(\"(This may take 2-5 minutes)\\n\")\n",
                "\n",
                "start = time.perf_counter()\n",
                "generate_large_dataset()\n",
                "elapsed = time.perf_counter() - start\n",
                "print(f\"\\nGeneration completed in {elapsed:.1f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 7: `chunked_statistics()`\n",
                "\n",
                "Calculate average price using chunking — **without loading the full file in RAM**.\n",
                "\n",
                "**Hints:**\n",
                "- Use `pd.read_csv(SALES_CSV, chunksize=500_000)` to get an iterator\n",
                "- Accumulate `total_sum` and `total_count` across chunks\n",
                "- Average = `total_sum / total_count`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chunked_statistics(chunksize: int = 500_000) -> dict:\n",
                "    \"\"\"\n",
                "    Calculate statistics using chunking over CSV.\n",
                "\n",
                "    Args:\n",
                "        chunksize: Number of rows per chunk\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with total_sum, total_count, and avg_price.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Initialize total_sum = 0, total_count = 0\n",
                "    # 2. Loop: for chunk in pd.read_csv(SALES_CSV, chunksize=chunksize):\n",
                "    #      total_sum += chunk['price'].sum()\n",
                "    #      total_count += len(chunk)\n",
                "    # 3. Calculate and return avg_price = total_sum / total_count\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run chunked statistics\n",
                "stats = chunked_statistics()\n",
                "\n",
                "if stats is None:\n",
                "    print(\"TODO: Implement chunked_statistics()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 8: `chunked_filter_save()`\n",
                "\n",
                "Filter only \"Electronics\" sales and save to Parquet — processing chunk by chunk.\n",
                "\n",
                "**Hints:**\n",
                "- Filter each chunk: `filtered = chunk[chunk['category'] == 'Electronics']`\n",
                "- Collect filtered chunks in a list\n",
                "- Concatenate and save: `pd.concat(results).to_parquet(...)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chunked_filter_save(chunksize: int = 500_000) -> int:\n",
                "    \"\"\"\n",
                "    Filter and save only Electronics sales using chunking.\n",
                "\n",
                "    Args:\n",
                "        chunksize: Number of rows per chunk\n",
                "\n",
                "    Returns:\n",
                "        Number of Electronics rows saved.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Initialize results = []\n",
                "    # 2. Loop: for chunk in pd.read_csv(SALES_CSV, chunksize=chunksize):\n",
                "    #      filtered = chunk[chunk['category'] == 'Electronics']\n",
                "    #      results.append(filtered)\n",
                "    # 3. Concatenate: electronics = pd.concat(results)\n",
                "    # 4. Save: electronics.to_parquet(ELECTRONICS_PARQUET, index=False)\n",
                "    # 5. Return len(electronics)\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter and save Electronics\n",
                "n_electronics = chunked_filter_save()\n",
                "\n",
                "if n_electronics is None:\n",
                "    print(\"TODO: Implement chunked_filter_save()\")\n",
                "else:\n",
                "    print(f\"\\nSaved {n_electronics:,} Electronics rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 9: `monitor_memory_chunking()`\n",
                "\n",
                "Monitor memory usage during chunked processing to prove it stays constant.\n",
                "\n",
                "**Hints:**\n",
                "- Use `psutil.Process(os.getpid()).memory_info().rss / 1024**2` for memory in MB\n",
                "- Record memory after each chunk\n",
                "- Plot with `plt.plot(mem_usage)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def monitor_memory_chunking(chunksize: int = 500_000) -> list:\n",
                "    \"\"\"\n",
                "    Monitor memory usage during chunked processing.\n",
                "\n",
                "    Args:\n",
                "        chunksize: Number of rows per chunk\n",
                "\n",
                "    Returns:\n",
                "        List of memory usage (MB) per chunk.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Get process: process = psutil.Process(os.getpid())\n",
                "    # 2. Initialize mem_usage = []\n",
                "    # 3. Loop: for chunk in pd.read_csv(SALES_CSV, chunksize=chunksize):\n",
                "    #      mem_mb = process.memory_info().rss / 1024**2\n",
                "    #      mem_usage.append(mem_mb)\n",
                "    #      chunk['price'].sum()  # Process\n",
                "    # 4. Plot and save to RESULTS_DIR / 'memoria_chunking.png'\n",
                "    # 5. Return mem_usage\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Monitor memory during chunking\n",
                "mem = monitor_memory_chunking()\n",
                "\n",
                "if mem is None:\n",
                "    print(\"TODO: Implement monitor_memory_chunking()\")\n",
                "else:\n",
                "    # Show the plot inline\n",
                "    plt.figure(figsize=(10, 5))\n",
                "    plt.plot(mem, 'b-o', markersize=3)\n",
                "    plt.xlabel('Chunk número')\n",
                "    plt.ylabel('Memoria (MB)')\n",
                "    plt.title('Memoria constante con chunking')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Exercise 2: Online Statistics — Welford's Algorithm (20 min)\n",
                "\n",
                "**Objetivo**: Implementar algoritmos streaming para calcular estadísticos sin almacenar todos los datos."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 10: `OnlineStats` class\n",
                "\n",
                "Implement Welford's algorithm for streaming mean, variance, and std.\n",
                "\n",
                "**Welford's algorithm:**\n",
                "```\n",
                "For each new value x:\n",
                "  count += 1\n",
                "  delta = x - mean\n",
                "  mean += delta / count\n",
                "  delta2 = x - mean    (note: uses UPDATED mean)\n",
                "  M2 += delta * delta2\n",
                "\n",
                "variance = M2 / count\n",
                "std = sqrt(variance)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class OnlineStats:\n",
                "    \"\"\"\n",
                "    Online (streaming) statistics using Welford's algorithm.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self):\n",
                "        self.count = 0\n",
                "        self.mean = 0.0\n",
                "        self.M2 = 0.0\n",
                "        self.min_val = float('inf')\n",
                "        self.max_val = float('-inf')\n",
                "\n",
                "    def update(self, x):\n",
                "        \"\"\"Implement Welford's algorithm.\"\"\"\n",
                "        # TODO: Implement this method\n",
                "        # 1. self.count += 1\n",
                "        # 2. delta = x - self.mean\n",
                "        # 3. self.mean += delta / self.count\n",
                "        # 4. delta2 = x - self.mean  (uses updated mean!)\n",
                "        # 5. self.M2 += delta * delta2\n",
                "        # 6. Update min_val and max_val\n",
                "        pass\n",
                "\n",
                "    def variance(self):\n",
                "        \"\"\"Return the population variance.\"\"\"\n",
                "        # TODO: return self.M2 / self.count (handle count < 2)\n",
                "        pass\n",
                "\n",
                "    def std(self):\n",
                "        \"\"\"Return the population standard deviation.\"\"\"\n",
                "        # TODO: return sqrt(variance)\n",
                "        pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 11: Validate OnlineStats against NumPy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def test_online_stats(n_rows: int = 100_000) -> dict:\n",
                "    \"\"\"\n",
                "    Test OnlineStats against NumPy for validation.\n",
                "\n",
                "    Args:\n",
                "        n_rows: Number of rows to read for testing\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with OnlineStats vs NumPy comparison.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Read first n_rows from CSV: pd.read_csv(SALES_CSV, nrows=n_rows)\n",
                "    # 2. Create OnlineStats and update with each price value\n",
                "    # 3. Compare with NumPy: chunk['price'].mean(), chunk['price'].std(ddof=0)\n",
                "    # 4. Assert np.isclose() for both mean and std\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate OnlineStats\n",
                "validation = test_online_stats()\n",
                "\n",
                "if validation is None:\n",
                "    print(\"TODO: Implement test_online_stats()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 12: Streaming statistics over full dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def streaming_stats_full(chunksize: int = 500_000) -> dict:\n",
                "    \"\"\"\n",
                "    Compute statistics over the full CSV using OnlineStats with chunking.\n",
                "\n",
                "    Args:\n",
                "        chunksize: Number of rows per chunk\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with final statistics.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Create OnlineStats instance\n",
                "    # 2. Loop: for chunk in pd.read_csv(SALES_CSV, chunksize=chunksize):\n",
                "    #      for value in chunk['price'].values:\n",
                "    #          stats.update(value)\n",
                "    # 3. Print and return results\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Streaming stats on full dataset\n",
                "print(\"Computing streaming statistics over full dataset...\")\n",
                "print(\"(This may take a few minutes)\\n\")\n",
                "\n",
                "start = time.perf_counter()\n",
                "full_stats = streaming_stats_full()\n",
                "elapsed = time.perf_counter() - start\n",
                "\n",
                "if full_stats is None:\n",
                "    print(\"TODO: Implement streaming_stats_full()\")\n",
                "else:\n",
                "    print(f\"\\nCompleted in {elapsed:.1f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Exercise 3: Practical Parallelization (25 min)\n",
                "\n",
                "**Objetivo**: Comparar threading, multiprocessing y secuencial."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 13: `create_partitions()`\n",
                "\n",
                "Split the dataset into 16 Parquet partition files.\n",
                "\n",
                "**Hints:**\n",
                "- Read the CSV, split into `n_partitions` pieces\n",
                "- Save each piece as `partitions/part_000.parquet`, `part_001.parquet`, etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_partitions(n_partitions: int = 16) -> list:\n",
                "    \"\"\"\n",
                "    Create partition files from the large dataset.\n",
                "\n",
                "    Args:\n",
                "        n_partitions: Number of partitions to create\n",
                "\n",
                "    Returns:\n",
                "        List of partition file paths.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Read CSV: df = pd.read_csv(SALES_CSV)\n",
                "    # 2. For each partition:\n",
                "    #      start_idx = i * len(df) // n_partitions\n",
                "    #      end_idx = (i + 1) * len(df) // n_partitions\n",
                "    #      chunk = df.iloc[start_idx:end_idx]\n",
                "    #      chunk.to_parquet(PARTITIONS_DIR / f'part_{i:03d}.parquet', index=False)\n",
                "    # 3. Return list of file paths\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create partitions\n",
                "print(\"Creating partitions...\")\n",
                "partition_files = create_partitions()\n",
                "\n",
                "if partition_files is None:\n",
                "    print(\"TODO: Implement create_partitions()\")\n",
                "else:\n",
                "    print(f\"Created {len(partition_files)} partition files\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 14: `benchmark_threading()`\n",
                "\n",
                "Compare sequential vs threaded **reading** of partition files.\n",
                "\n",
                "**Hints:**\n",
                "- Sequential: `[pd.read_parquet(f) for f in files]`\n",
                "- Threaded: `ThreadPoolExecutor(max_workers=8)` with `executor.map(pd.read_parquet, files)`\n",
                "- Reading files is I/O-bound → threads help!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_threading(n_workers: int = 8) -> dict:\n",
                "    \"\"\"\n",
                "    Benchmark sequential vs threaded reading of partition files.\n",
                "\n",
                "    Args:\n",
                "        n_workers: Number of threads to use\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with sequential/threaded times and speedup.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Get file list: files = sorted(glob.glob(str(PARTITIONS_DIR / '*.parquet')))\n",
                "    # 2. Sequential: [pd.read_parquet(f) for f in files]\n",
                "    # 3. Threaded: ThreadPoolExecutor(max_workers=n_workers)\n",
                "    # 4. Print times and speedup\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark threading\n",
                "thread_bench = benchmark_threading()\n",
                "\n",
                "if thread_bench is None:\n",
                "    print(\"TODO: Implement benchmark_threading()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 15: `benchmark_multiprocessing()`\n",
                "\n",
                "Compare sequential vs multiprocessing for **heavy computation** on each partition.\n",
                "\n",
                "**Hints:**\n",
                "- The `heavy_process()` function does computation (sqrt, log1p, groupby)\n",
                "- Sequential: `[heavy_process(f) for f in files]`\n",
                "- Parallel: `ProcessPoolExecutor(max_workers=4)`\n",
                "- CPU-bound work → processes help!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def heavy_process(filepath):\n",
                "    \"\"\"Process a partition: read, transform, aggregate.\"\"\"\n",
                "    df = pd.read_parquet(filepath)\n",
                "    df['score'] = np.sqrt(df['price']) * np.log1p(df['quantity'])\n",
                "    return df.groupby('category')['score'].agg(['mean', 'sum', 'count'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_multiprocessing(n_workers: int = 4) -> dict:\n",
                "    \"\"\"\n",
                "    Benchmark sequential vs multiprocessing for heavy computation.\n",
                "\n",
                "    Args:\n",
                "        n_workers: Number of processes to use\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with sequential/parallel times and speedup.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Get file list\n",
                "    # 2. Sequential: [heavy_process(f) for f in files]\n",
                "    # 3. Parallel: ProcessPoolExecutor(max_workers=n_workers)\n",
                "    # 4. Print times and speedup\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark multiprocessing\n",
                "proc_bench = benchmark_multiprocessing()\n",
                "\n",
                "if proc_bench is None:\n",
                "    print(\"TODO: Implement benchmark_multiprocessing()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 16: `benchmark_workers_scaling()`\n",
                "\n",
                "Experiment: vary the number of workers (1, 2, 4, 8) and compare speedup vs linear ideal.\n",
                "\n",
                "**Hints:**\n",
                "- Run `heavy_process` sequentially first (baseline)\n",
                "- Then run with 1, 2, 4, 8 workers using ProcessPoolExecutor\n",
                "- Plot actual speedup vs ideal linear speedup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_workers_scaling(max_workers: int = 8) -> dict:\n",
                "    \"\"\"\n",
                "    Experiment: vary number of workers and measure speedup.\n",
                "\n",
                "    Args:\n",
                "        max_workers: Maximum number of workers to test\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with worker count → speedup mapping.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Run sequential baseline\n",
                "    # 2. For n_workers in [1, 2, 4, max_workers]:\n",
                "    #      Run with ProcessPoolExecutor(max_workers=n_workers)\n",
                "    #      Record speedup = seq_time / elapsed\n",
                "    # 3. Plot: actual speedup vs ideal linear\n",
                "    # 4. Save plot to RESULTS_DIR / 'speedup_workers.png'\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark worker scaling\n",
                "print(\"Benchmarking worker scaling...\")\n",
                "print(\"(This may take a few minutes)\\n\")\n",
                "\n",
                "scaling = benchmark_workers_scaling()\n",
                "\n",
                "if scaling is None:\n",
                "    print(\"TODO: Implement benchmark_workers_scaling()\")\n",
                "else:\n",
                "    # Show the plot inline\n",
                "    plt.figure(figsize=(8, 5))\n",
                "    plt.plot(list(scaling.keys()), list(scaling.values()), 'bo-', linewidth=2, markersize=8)\n",
                "    plt.plot([1, max(scaling.keys())], [1, max(scaling.keys())], 'r--', label='Lineal ideal', alpha=0.7)\n",
                "    plt.xlabel('Workers')\n",
                "    plt.ylabel('Speedup')\n",
                "    plt.legend()\n",
                "    plt.title('Speedup vs Número de Workers')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Exercise 4: Complete Pipeline — Out-of-Core + Parallel (20 min)\n",
                "\n",
                "**Objetivo**: Combinar chunking y paralelización en un pipeline real."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 17: `process_partition()`\n",
                "\n",
                "Process a single partition: read, transform, aggregate.\n",
                "\n",
                "**Hints:**\n",
                "- Calculate `revenue = price * quantity`\n",
                "- Create `price_bin` with `pd.cut(df['price'], bins=[0, 50, 200, 500, 1000], labels=[...])`\n",
                "- Group by `['category', 'price_bin']` and aggregate revenue and quantity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_partition(filepath):\n",
                "    \"\"\"\n",
                "    Process a partition: read, transform, aggregate.\n",
                "\n",
                "    Args:\n",
                "        filepath: Path to the Parquet partition file\n",
                "\n",
                "    Returns:\n",
                "        Aggregated DataFrame with revenue statistics by category and price_bin.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Read: df = pd.read_parquet(filepath)\n",
                "    # 2. Transform:\n",
                "    #    df['revenue'] = df['price'] * df['quantity']\n",
                "    #    df['price_bin'] = pd.cut(df['price'], bins=[0, 50, 200, 500, 1000],\n",
                "    #                            labels=['low', 'mid', 'high', 'premium'])\n",
                "    # 3. Aggregate:\n",
                "    #    result = df.groupby(['category', 'price_bin'], observed=True).agg({\n",
                "    #        'revenue': ['sum', 'mean', 'count'],\n",
                "    #        'quantity': 'sum'\n",
                "    #    })\n",
                "    # 4. Return result\n",
                "    pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TODO 18: `run_parallel_pipeline()`\n",
                "\n",
                "Run the complete pipeline: process all partitions in parallel, then combine results.\n",
                "\n",
                "**Hints:**\n",
                "- Sequential: `[process_partition(f) for f in files]`\n",
                "- Parallel: `ProcessPoolExecutor(max_workers=4)` with `executor.map()`\n",
                "- Combine: `pd.concat(results).groupby(level=[0, 1]).sum()`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_parallel_pipeline(n_workers: int = 4) -> dict:\n",
                "    \"\"\"\n",
                "    Run the complete parallel pipeline and compare with sequential.\n",
                "\n",
                "    Args:\n",
                "        n_workers: Number of worker processes\n",
                "\n",
                "    Returns:\n",
                "        Dictionary with sequential/parallel times, speedup, and final results.\n",
                "    \"\"\"\n",
                "    # TODO: Implement this function\n",
                "    # 1. Get file list from PARTITIONS_DIR\n",
                "    # 2. Sequential pipeline:\n",
                "    #      results = [process_partition(f) for f in files]\n",
                "    #      final = pd.concat(results).groupby(level=[0, 1]).sum()\n",
                "    # 3. Parallel pipeline:\n",
                "    #      with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
                "    #          results = list(executor.map(process_partition, files))\n",
                "    #      final = pd.concat(results).groupby(level=[0, 1]).sum()\n",
                "    # 4. Print times, speedup, and final results\n",
                "    pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run complete pipeline\n",
                "print(\"Running complete pipeline (sequential vs parallel)...\")\n",
                "print(\"(This may take a few minutes)\\n\")\n",
                "\n",
                "pipeline_results = run_parallel_pipeline()\n",
                "\n",
                "if pipeline_results is None:\n",
                "    print(\"TODO: Implement run_parallel_pipeline()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Save Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Collect all metrics\n",
                "metrics = {\n",
                "    'exercise_0': {\n",
                "        'read_benchmark': read_bench if 'read_bench' in dir() and read_bench else {},\n",
                "        'projection': proj_bench if 'proj_bench' in dir() and proj_bench else {},\n",
                "        'iter_batches': batch_results if 'batch_results' in dir() and batch_results else {},\n",
                "    },\n",
                "    'exercise_1': {\n",
                "        'chunked_stats': stats if 'stats' in dir() and stats else {},\n",
                "        'electronics_rows': n_electronics if 'n_electronics' in dir() and n_electronics else 0,\n",
                "    },\n",
                "    'exercise_2': {\n",
                "        'validation': validation if 'validation' in dir() and validation else {},\n",
                "        'full_stats': full_stats if 'full_stats' in dir() and full_stats else {},\n",
                "    },\n",
                "    'exercise_3': {\n",
                "        'threading': thread_bench if 'thread_bench' in dir() and thread_bench else {},\n",
                "        'multiprocessing': proc_bench if 'proc_bench' in dir() and proc_bench else {},\n",
                "        'scaling': scaling if 'scaling' in dir() and scaling else {},\n",
                "    },\n",
                "    'exercise_4': pipeline_results if 'pipeline_results' in dir() and pipeline_results else {},\n",
                "}\n",
                "\n",
                "with open(METRICS_PATH, 'w') as f:\n",
                "    json.dump(metrics, f, indent=2, default=str)\n",
                "\n",
                "print(f\"\\n✅ Metrics saved to {METRICS_PATH}\")\n",
                "print(json.dumps(metrics, indent=2, default=str))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Key Takeaways\n",
                "\n",
                "1. **PyArrow > Pandas for I/O** — `pq.read_table()` avoids conversion overhead\n",
                "2. **Projection pushdown** — read only needed columns, save time and memory\n",
                "3. **Chunking keeps memory constant** — process any file size with fixed RAM\n",
                "4. **Welford's algorithm** — compute statistics in a single pass without storing data\n",
                "5. **Threading for I/O** — parallelize file reading with threads\n",
                "6. **Multiprocessing for CPU** — parallelize computation with processes\n",
                "7. **Speedup is sub-linear** — Amdahl's Law limits gains from parallelization\n",
                "8. **Combine everything** — chunking + parallelization = scalable pipelines\n",
                "\n",
                "---\n",
                "\n",
                "**Questions?** Check the [Tips & Reference Guide](../docs/labs/lab06_guide.md) or ask your instructor."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}