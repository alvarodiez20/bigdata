{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 03: Data Types\n",
    "\n",
    "**Course:** Big Data\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ‘¤ Student Information\n",
    "\n",
    "**Name:** `Your Name Here`\n",
    "\n",
    "**Date:** `DD/MM/YYYY`\n",
    "\n",
    "---\n",
    "\n",
    "**Goal:** Master data type optimization to achieve significant memory and performance improvements.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Measure Memory Usage**: Accurately measure DataFrame memory consumption\n",
    "2. **Analyze Data Ranges**: Identify value ranges to determine optimal types\n",
    "3. **Optimize Data Types**: Reduce memory usage 5-10x through smart dtype selection\n",
    "4. **Measure Performance Impact**: Benchmark the speed improvements from type optimization\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Fill in your information above** before starting the lab\n",
    "2. Read each cell carefully before running it\n",
    "3. Implement the **TODO functions** when you see them\n",
    "4. Run cells **from top to bottom** (Shift+Enter)\n",
    "5. Check that output makes sense after each cell\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## ðŸ“š Libraries Used in This Lab\n",
    "\n",
    "### Core Libraries\n",
    "\n",
    "- **`pandas`** - DataFrame operations and I/O\n",
    "- **`numpy`** - Random data generation\n",
    "- **`time`** - Performance measurement\n",
    "\n",
    "### Why Focus on Data Types?\n",
    "\n",
    "**Real-world example**: A 100M row sales dataset\n",
    "\n",
    "| Approach | RAM Usage | Groupby Time |\n",
    "|----------|-----------|-------------|\n",
    "| Naive (default types) | 80 GB | 45 sec |\n",
    "| Optimized (proper types) | 8 GB | 5 sec |\n",
    "\n",
    "**That's 10x less memory and 9x faster!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directories\n",
    "DATA_RAW = Path(\"../data/raw\")\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "\n",
    "# File paths for this lab\n",
    "ECOMMERCE_CSV = DATA_RAW / \"ecommerce_5m.csv\"\n",
    "METRICS_PATH = RESULTS_DIR / \"lab03_metrics.json\"\n",
    "\n",
    "# Ensure directories exist\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Paths defined:\")\n",
    "print(f\"  Source CSV: {ECOMMERCE_CSV}\")\n",
    "print(f\"  Metrics: {METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section A. Dataset Generation (15 min)\n",
    "\n",
    "First, we generate a synthetic e-commerce dataset with 5 million rows.\n",
    "\n",
    "**Columns:**\n",
    "- `order_id`: Unique order identifier (0 to 4,999,999)\n",
    "- `product_id`: Product ID (1-50,000)\n",
    "- `category`: Product category (15 unique values)\n",
    "- `price`: Product price (0.01-999.99)\n",
    "- `quantity`: Quantity ordered (1-100)\n",
    "- `country`: Customer country (30 unique values)\n",
    "- `timestamp`: Order timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### TODO 1: `generate_ecommerce_data()`\n",
    "\n",
    "Generate a synthetic e-commerce dataset.\n",
    "\n",
    "**ðŸ’¡ Hints:**\n",
    "- Use `np.random.seed(seed)` for reproducibility\n",
    "- Use `np.arange(n_rows)` for order_id\n",
    "- Use `np.random.randint()` for integer columns\n",
    "- Use `np.random.choice()` for category/country columns\n",
    "- Use `np.random.uniform()` for price\n",
    "- Use `pd.date_range()` for timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ecommerce_data(path: Path, n_rows: int = 5_000_000, seed: int = 42) -> dict:\n",
    "    \"\"\"\n",
    "    Generate a synthetic e-commerce dataset.\n",
    "    \n",
    "    Args:\n",
    "        path: Where to save the CSV\n",
    "        n_rows: Number of rows (default 5 million)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with: {\"rows\": int, \"cols\": int, \"size_mb\": float}\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Step 1: Set random seed\n",
    "    # Step 2: Define categories list (15 items)\n",
    "    # Step 3: Define countries list (30 items)\n",
    "    # Step 4: Generate each column using numpy\n",
    "    # Step 5: Create DataFrame\n",
    "    # Step 6: Save to CSV (index=False)\n",
    "    # Step 7: Return metadata dict\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset (only if it doesn't exist)\n",
    "if not ECOMMERCE_CSV.exists():\n",
    "    print(\"Generating 5 million row e-commerce dataset...\")\n",
    "    print(\"(This may take 1-2 minutes)\\n\")\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    metadata = generate_ecommerce_data(ECOMMERCE_CSV, n_rows=5_000_000)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"Generated in {elapsed:.1f} seconds\")\n",
    "    print(f\"Rows: {metadata['rows']:,}\")\n",
    "    print(f\"Size: {metadata['size_mb']:.1f} MB\")\n",
    "else:\n",
    "    size_mb = ECOMMERCE_CSV.stat().st_size / 1e6\n",
    "    print(f\"Dataset already exists: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section B. Baseline Measurement (15 min)\n",
    "\n",
    "Let's see how much memory pandas uses with default dtypes.\n",
    "\n",
    "### Part B1: Load with Default Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with default dtypes\n",
    "print(\"Loading CSV with default dtypes...\")\n",
    "start = time.perf_counter()\n",
    "df_baseline = pd.read_csv(ECOMMERCE_CSV)\n",
    "load_time_baseline = time.perf_counter() - start\n",
    "\n",
    "print(f\"Load time: {load_time_baseline:.2f} seconds\")\n",
    "print(f\"\\nDataFrame shape: {df_baseline.shape}\")\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "print(df_baseline.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### TODO 2: `measure_memory()`\n",
    "\n",
    "Measure memory usage of a DataFrame.\n",
    "\n",
    "**ðŸ’¡ Hints:**\n",
    "- Use `df.memory_usage(deep=True)` for accurate measurement\n",
    "- The `deep=True` flag is essential for object (string) columns\n",
    "- Convert bytes to MB by dividing by 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Measure memory usage of a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to measure\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'total_mb': total memory in MB\n",
    "        - 'columns': dict with per-column info (dtype, memory_mb, nunique)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. Use df.memory_usage(deep=True) to get memory per column\n",
    "    # 2. Calculate total memory in MB\n",
    "    # 3. For each column, record dtype, memory, and nunique count\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure baseline memory\n",
    "baseline_memory = measure_memory(df_baseline)\n",
    "print(f\"\\nTotal memory: {baseline_memory['total_mb']:.2f} MB\")\n",
    "print(\"\\nPer-column breakdown:\")\n",
    "for col, info in baseline_memory['columns'].items():\n",
    "    print(f\"  {col}: {info['dtype']} - {info['memory_mb']:.2f} MB ({info['nunique']:,} unique)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Key Insight: Default Types Are Wasteful\n",
    "\n",
    "Notice how pandas uses:\n",
    "- `int64` (8 bytes) for ALL integers, even small ones\n",
    "- `float64` (8 bytes) for ALL floats\n",
    "- `object` (~50+ bytes per value) for strings\n",
    "\n",
    "This is **extremely wasteful** when your data has limited ranges!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Section C. Type Analysis & Optimization (30 min)\n",
    "\n",
    "### Part C1: Analyze Value Ranges\n",
    "\n",
    "To choose optimal types, we need to understand our data's actual value ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### TODO 3: `analyze_column_ranges()`\n",
    "\n",
    "Analyze each column to determine the optimal type.\n",
    "\n",
    "**ðŸ’¡ Hints:**\n",
    "- For numeric columns: use `.min()`, `.max()`\n",
    "- For string columns: use `.nunique()`, `.str.len().max()`\n",
    "- Compare ranges against the Integer Types Reference table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_column_ranges(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze value ranges for each column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis for each column:\n",
    "        - For numeric: {'min': x, 'max': y, 'nunique': n}\n",
    "        - For string: {'nunique': n, 'max_len': l, 'sample': [...]}\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # For each column, determine if it's numeric or string\n",
    "    # Numeric: min, max, nunique\n",
    "    # String: nunique, max string length, sample values\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze value ranges\n",
    "ranges = analyze_column_ranges(df_baseline)\n",
    "\n",
    "print(\"Column Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "for col, info in ranges.items():\n",
    "    if 'min' in info:\n",
    "        print(f\"{col}: {info['min']} to {info['max']} ({info['nunique']:,} unique)\")\n",
    "    else:\n",
    "        print(f\"{col}: {info['nunique']} unique, max length {info['max_len']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### ðŸ“Š Integer Type Reference Table\n",
    "\n",
    "| Type | Min | Max | Bytes |\n",
    "|------|-----|-----|-------|\n",
    "| int8 | -128 | 127 | 1 |\n",
    "| **uint8** | 0 | **255** | **1** |\n",
    "| int16 | -32,768 | 32,767 | 2 |\n",
    "| **uint16** | 0 | **65,535** | **2** |\n",
    "| int32 | -2.1B | 2.1B | 4 |\n",
    "| **uint32** | 0 | **4.3B** | **4** |\n",
    "| int64 | -9.2Q | 9.2Q | 8 |\n",
    "\n",
    "**Rule**: Use the **smallest** type that fits your data range!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Part C2: Determine Optimal Types\n",
    "\n",
    "Based on your analysis, fill in the optimal types:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### TODO 4: `get_optimal_dtypes()`\n",
    "\n",
    "Return a dictionary mapping column names to optimal dtype strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_dtypes() -> dict:\n",
    "    \"\"\"\n",
    "    Return the optimal dtypes for the ecommerce dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping column names to dtype strings\n",
    "    \"\"\"\n",
    "    # TODO: Fill in the optimal types based on your analysis\n",
    "    return {\n",
    "        'order_id': '???',      # 0 to 5M - which int type?\n",
    "        'product_id': '???',    # 1 to 50000 - which int type?\n",
    "        'category': '???',      # 15 unique strings - category?\n",
    "        'price': '???',         # 0.01 to 999.99 - float32 or float64?\n",
    "        'quantity': '???',      # 1 to 100 - which int type?\n",
    "        'country': '???',       # 30 unique strings - category?\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_dtypes = get_optimal_dtypes()\n",
    "print(\"Optimal dtypes:\")\n",
    "for col, dtype in optimal_dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Part C3: Load with Optimized Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### TODO 5: `load_with_optimized_dtypes()`\n",
    "\n",
    "Load the CSV with optimized dtypes.\n",
    "\n",
    "**ðŸ’¡ Hints:**\n",
    "- Pass `dtype=` parameter to `pd.read_csv()`\n",
    "- Use `parse_dates=['timestamp']` for the timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_optimized_dtypes(path: Path, dtypes: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV with optimized dtypes.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to CSV file\n",
    "        dtypes: Dictionary mapping column names to dtype strings\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with optimized dtypes\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Use pd.read_csv with dtype and parse_dates parameters\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with optimized dtypes\n",
    "print(\"Loading CSV with optimized dtypes...\")\n",
    "start = time.perf_counter()\n",
    "df_optimized = load_with_optimized_dtypes(ECOMMERCE_CSV, optimal_dtypes)\n",
    "load_time_optimized = time.perf_counter() - start\n",
    "\n",
    "print(f\"Load time: {load_time_optimized:.2f} seconds\")\n",
    "print(f\"\\nColumn dtypes:\")\n",
    "print(df_optimized.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure optimized memory\n",
    "optimized_memory = measure_memory(df_optimized)\n",
    "\n",
    "print(f\"Baseline memory: {baseline_memory['total_mb']:.2f} MB\")\n",
    "print(f\"Optimized memory: {optimized_memory['total_mb']:.2f} MB\")\n",
    "print(f\"\\nReduction: {baseline_memory['total_mb'] / optimized_memory['total_mb']:.1f}x\")\n",
    "\n",
    "print(\"\\nPer-column comparison:\")\n",
    "for col in baseline_memory['columns']:\n",
    "    before = baseline_memory['columns'][col]['memory_mb']\n",
    "    after = optimized_memory['columns'][col]['memory_mb']\n",
    "    reduction = before / after if after > 0 else 0\n",
    "    print(f\"  {col}: {before:.1f} MB â†’ {after:.1f} MB ({reduction:.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Key Insight: Category dtype\n",
    "\n",
    "The `category` dtype is especially powerful for repeated strings:\n",
    "\n",
    "```python\n",
    "# Internally stored as:\n",
    "# Dictionary: {0: 'Electronics', 1: 'Clothing', ...}\n",
    "# Codes: [0, 1, 0, 2, 1, ...]  (small integers!)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Memory scales with **unique values**, not row count\n",
    "- Groupby operates on integers, not strings\n",
    "- String comparisons use integer codes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Section D. Performance Impact (15 min)\n",
    "\n",
    "Smaller types aren't just about memory â€” they're also **faster**!\n",
    "\n",
    "### TODO 6: `benchmark_operation()`\n",
    "\n",
    "Benchmark an operation on both baseline and optimized DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(df_baseline: pd.DataFrame, df_optimized: pd.DataFrame,\n",
    "                        operation: str) -> dict:\n",
    "    \"\"\"\n",
    "    Benchmark an operation on baseline vs optimized DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df_baseline: DataFrame with default dtypes\n",
    "        df_optimized: DataFrame with optimized dtypes\n",
    "        operation: One of 'groupby_sum', 'filter', 'sort'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with baseline_sec, optimized_sec, speedup\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # For 'groupby_sum': df.groupby('category')['price'].sum()\n",
    "    # For 'filter': df[df['country'] == 'Spain']\n",
    "    # For 'sort': df.sort_values('price')\n",
    "    # Time each operation on both DataFrames and calculate speedup\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark groupby operation\n",
    "print(\"Benchmarking operations...\\n\")\n",
    "\n",
    "groupby_results = benchmark_operation(df_baseline, df_optimized, 'groupby_sum')\n",
    "print(f\"Groupby Sum:\")\n",
    "print(f\"  Baseline: {groupby_results['baseline_sec']:.4f} sec\")\n",
    "print(f\"  Optimized: {groupby_results['optimized_sec']:.4f} sec\")\n",
    "print(f\"  Speedup: {groupby_results['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark filter operation\n",
    "filter_results = benchmark_operation(df_baseline, df_optimized, 'filter')\n",
    "print(f\"Filter (country == 'Spain'):\")\n",
    "print(f\"  Baseline: {filter_results['baseline_sec']:.4f} sec\")\n",
    "print(f\"  Optimized: {filter_results['optimized_sec']:.4f} sec\")\n",
    "print(f\"  Speedup: {filter_results['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark sort operation\n",
    "sort_results = benchmark_operation(df_baseline, df_optimized, 'sort')\n",
    "print(f\"Sort by price:\")\n",
    "print(f\"  Baseline: {sort_results['baseline_sec']:.4f} sec\")\n",
    "print(f\"  Optimized: {sort_results['optimized_sec']:.4f} sec\")\n",
    "print(f\"  Speedup: {sort_results['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### TODO 7: `calculate_savings()`\n",
    "\n",
    "Calculate the total memory and performance savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_savings(baseline_memory: dict, optimized_memory: dict,\n",
    "                      benchmark_results: list) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate total savings from optimization.\n",
    "    \n",
    "    Args:\n",
    "        baseline_memory: Memory info for baseline DataFrame\n",
    "        optimized_memory: Memory info for optimized DataFrame\n",
    "        benchmark_results: List of benchmark result dicts\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - memory_saved_mb: MB of memory saved\n",
    "        - memory_reduction_factor: how many times smaller\n",
    "        - avg_speedup: average speedup across all operations\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # Calculate memory saved and average speedup\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total savings\n",
    "savings = calculate_savings(\n",
    "    baseline_memory, \n",
    "    optimized_memory,\n",
    "    [groupby_results, filter_results, sort_results]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TOTAL SAVINGS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Memory saved: {savings['memory_saved_mb']:.1f} MB\")\n",
    "print(f\"Memory reduction: {savings['memory_reduction_factor']:.1f}x smaller\")\n",
    "print(f\"Average speedup: {savings['avg_speedup']:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section E. Reflection & Save Results (15 min)\n",
    "\n",
    "### Reflection\n",
    "\n",
    "**Your task:** Write a short reflection (3-5 sentences) answering:\n",
    "\n",
    "1. What was the biggest memory reduction you achieved on a single column?\n",
    "2. Which dtype change had the most impact: integer downcasting or using `category`?\n",
    "3. What will you do differently when working with large datasets in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your reflection here\n",
    "reflection = \"\"\"\n",
    "Replace this text with your reflection.\n",
    "Think about what you learned about data types.\n",
    "What will you do differently in your future projects?\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Your reflection:\")\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "results = {\n",
    "    \"lab\": \"03_data_types\",\n",
    "    \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "    \"dataset\": {\n",
    "        \"rows\": len(df_optimized),\n",
    "        \"columns\": len(df_optimized.columns),\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"baseline_mb\": baseline_memory['total_mb'],\n",
    "        \"optimized_mb\": optimized_memory['total_mb'],\n",
    "        \"reduction_factor\": round(baseline_memory['total_mb'] / optimized_memory['total_mb'], 2),\n",
    "        \"saved_mb\": round(baseline_memory['total_mb'] - optimized_memory['total_mb'], 2),\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"groupby_speedup\": groupby_results['speedup'],\n",
    "        \"filter_speedup\": filter_results['speedup'],\n",
    "        \"sort_speedup\": sort_results['speedup'],\n",
    "        \"avg_speedup\": savings['avg_speedup'],\n",
    "    },\n",
    "    \"dtypes_used\": optimal_dtypes,\n",
    "    \"reflection\": reflection,\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(METRICS_PATH, \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"âœ“ Results saved to: {METRICS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Lab Complete!\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Default types are wasteful**: Pandas uses int64/float64/object by default\n",
    "2. **Analyze before optimizing**: Check min/max/nunique to choose the right type\n",
    "3. **Integer sizing matters**: Use uint8/uint16/uint32 based on actual ranges\n",
    "4. **Category is powerful**: Perfect for repeated strings (<50% unique values)\n",
    "5. **Smaller = Faster**: Reduced memory leads to faster operations\n",
    "\n",
    "### Optimization Checklist\n",
    "\n",
    "- âœ… Use `df.memory_usage(deep=True)` to measure accurately\n",
    "- âœ… Check `.min()` and `.max()` for numeric columns\n",
    "- âœ… Check `.nunique()` for potential `category` columns\n",
    "- âœ… Use smallest int type that fits your data\n",
    "- âœ… Use `category` for strings with <50% unique values\n",
    "- âœ… Use `float32` unless you need high precision\n",
    "- âœ… Specify dtypes when reading CSV with `dtype=`\n",
    "\n",
    "### Files to Submit\n",
    "\n",
    "1. `notebooks/lab03_data_types.ipynb` (this notebook)\n",
    "2. `results/lab03_metrics.json`\n",
    "\n",
    "---\n",
    "\n",
    "**Next Lab**: We'll explore efficient storage formats (Parquet, Feather) and partitioning strategies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
